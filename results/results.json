{"summary":{},"version":"2.0.0","results":[{"version":1,"platform":"K8S","category":"Workload Isolation","resource":"Pods","title":"Ensure pods only run in dedicated namespaces","description":"By default, user-managed resources will be placed in the `default` namespace.  This makes it difficult to properly define policies for RBAC permissions, service account usage, network policies, and more.  Creating dedicated namespaces and running workloads and supporting resources in each helps support proper API server permissions separation and network microsegmentation.","remediation":"Create dedicated namespaces for each type of related workload, and migrate those resources into those namespaces.  Ensure that RBAC permissions are not granted at the cluster scope but per namespace for the application owners at each namespace level.","validation":"Run `kubectl get all` in the `default`, `kube-public`, and if present, `kube-node-lease` namespaces.  There should only be the `kubernetes` service.","severity":0.2,"effort":0.3,"references":[{"url":"https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/","ref":"Kubernetes Namespaces"}],"resources":[{"status":"failed","resource":"default namespace"},{"status":"passed","resource":"kube-public namespace"},{"status":"passed","resource":"kube-node-lease namespace"}],"result":{"status":"failed","passed":2,"total":3}},{"version":1,"platform":"K8S","category":"Management and Governance","resource":"Pods","title":"Ensure the Kubernetes Dashboard is not present","description":"While the Kubernetes dashboard is not inherently insecure on its own, it is often coupled with a misconfiguration of RBAC permissions that can unintentionally overgrant access and is not commonly protected with `NetworkPolicies` preventing all pods from being able to reach it.  In increasingly rare circumstances, the Kubernetes dashboard is exposed publicly to the Internet.","remediation":"Instead of running a workload inside the cluster to display a UI, leverage the cloud provider's UI for listing/managing workloads or consider a tool such as Octant running on local systems.  Run `kubectl get pods --all-namespaces -l k8s-app=kubernetes-dashboard` to find pods part of deployments and use kubectl to delete those deployments.","validation":"Running `kubectl get pods --all-namespaces -l k8s-app=kubernetes-dashboard` should not return any pods.","severity":0.3,"effort":0.1,"references":[{"url":"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/","ref":"Kubernetes Dashboard"}],"resources":[{"status":"passed","resource":"Dashboard"}],"result":{"status":"passed","passed":1,"total":1}},{"version":1,"platform":"K8S","category":"Addon Security","resource":"Pods","title":"Ensure Tiller (Helm v2) is not deployed","description":"Helm version 1.x and 2.x rely on an in-cluster deployment named `Tiller` to handle lifecycle management of Kubernetes application bundles called `charts`.  The `Tiller` deployment is commonly granted elevated privileges to be able to carry out creation/deletion of resources contained inside `charts`, and it exposes a gRPC port on TCP/44134 without authentication or authorization, by default.  This combination was common, and it afforded a simple and direct path to escalation to cluster-admin from any pod in the cluster.  Now that Helm v3 no longer relies on an in-cluster component, `Tiller` is a signal that the cluster administrators have not upgraded to the more secure version.","remediation":"Refer to https://helm.sh/docs/topics/v2_v3_migration/ for guidance on migrating away from `Tiller`.  For new cluster deployments, use Helm v3 and above going forward.","validation":"Run `kubectl get pods --all-namespaces -o name | grep tiller` and validate that no pods starting with the name `tiller-deploy-****` exist.","severity":1.0,"effort":0.2,"references":[{"url":"https://helm.sh","ref":"Helm"},{"url":"https://helm.sh/docs/faq/#removal-of-tiller","ref":"Tiller v2"},{"url":"https://helm.sh/docs/topics/v2_v3_migration/","ref":"Helm Migration from v2 to v3"},{"url":"https://engineering.bitnami.com/articles/helm-security.html","ref":"Misusing Tiller"}],"resources":[{"status":"passed","resource":"default/tiller-deploy"},{"status":"passed","resource":"development/tiller-deploy"},{"status":"passed","resource":"kube-node-lease/tiller-deploy"},{"status":"passed","resource":"kube-public/tiller-deploy"},{"status":"passed","resource":"kube-system/tiller-deploy"},{"status":"passed","resource":"production/tiller-deploy"}],"result":{"status":"passed","passed":6,"total":6}},{"version":1,"platform":"K8S","category":"Management and Governance","resource":"Pods","title":"Ensure all containers refer to a specific version tag not named latest","description":"When referring to a container image stored in a registry, it's common practice for the owner of the image to tag the most recent image with a semver tag and also the `latest` tag when uploading it.  This is a convenenience for users wanting to work with the most up-to-date image, but it presents an opportunity for inconsistencies inside Kubernetes.  If a deployment with more than one replica references an image with the tag `latest`, the underlying node will pull and run that image at that time.  If the image in the registry is updated with a new `latest` image and the deployment scales the number of replicas such that a new worker node is to run it, that node will potentially pull the newer `latest` image.","remediation":"Review all deployments and pod specifications, and modify any that reference the `latest` tag to use a specific version tag or even the `sha256` hash.  Consider enforcing this practice early with a validation step in the CI/CD pipeline and enforcing the policy with OPA/Gatekeeper or other policy-based admission controller inside the cluster.","validation":"Run `kubectl get po -A -ojsonpath='{..image}' | kubectl get pods --all-namespaces -o jsonpath='{..image}' |tr -s '[[:space:]]' '\n' | sort | uniq -c | grep latest` and ensure no images reference the `latest` tag.","severity":0.5,"effort":0.2,"references":[{"url":"https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/","ref":"Kubectl List Images"},{"url":"https://kubernetes.io/docs/concepts/configuration/overview/#container-images","ref":"Kubernetes Configuration Best Practices"}],"resources":[{"status":"failed","resource":"default/kube-bench-6vplv"},{"status":"failed","resource":"default/kube-bench-d5jx2"},{"status":"failed","resource":"default/kube-bench-g9gwh"},{"status":"failed","resource":"default/kube-bench-mqzkw"},{"status":"passed","resource":"default/kube-hunter-json-4bsvf"},{"status":"passed","resource":"default/kube-hunter-json-4hwjm"},{"status":"passed","resource":"default/kube-hunter-json-7z9wx"},{"status":"passed","resource":"default/kube-hunter-json-d5zj7"},{"status":"passed","resource":"default/kube-hunter-json-f74gb"},{"status":"passed","resource":"default/kube-hunter-json-f8xn5"},{"status":"passed","resource":"default/kube-hunter-json-g8bst"},{"status":"passed","resource":"default/kube-hunter-json-gqr7n"},{"status":"passed","resource":"default/kube-hunter-json-gzbsd"},{"status":"passed","resource":"default/kube-hunter-json-jfcws"},{"status":"passed","resource":"default/kube-hunter-json-jqptd"},{"status":"passed","resource":"default/kube-hunter-json-jv4j5"},{"status":"passed","resource":"default/kube-hunter-json-kjcjr"},{"status":"passed","resource":"default/kube-hunter-json-n2l5v"},{"status":"passed","resource":"default/kube-hunter-json-pftp6"},{"status":"passed","resource":"default/kube-hunter-json-psfgg"},{"status":"passed","resource":"default/kube-hunter-json-q6hp9"},{"status":"passed","resource":"default/kube-hunter-json-s2lpv"},{"status":"passed","resource":"default/kube-hunter-json-test-q8gt8"},{"status":"passed","resource":"default/kube-hunter-json-zfprc"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-5sfmz"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-h8lzn"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-p4r2d"},{"status":"passed","resource":"kube-system/calico-kube-controllers-dc4469c7f-d4rmp"},{"status":"passed","resource":"kube-system/calico-node-65r2f"},{"status":"passed","resource":"kube-system/calico-node-kqqc6"},{"status":"passed","resource":"kube-system/calico-node-lhqpq"},{"status":"passed","resource":"kube-system/coredns-66bff467f8-ft8gh"},{"status":"passed","resource":"kube-system/coredns-66bff467f8-mrnpx"},{"status":"passed","resource":"kube-system/etcd-kv-master-0"},{"status":"passed","resource":"kube-system/kube-apiserver-kv-master-0"},{"status":"passed","resource":"kube-system/kube-controller-manager-kv-master-0"},{"status":"passed","resource":"kube-system/kube-proxy-l8vng"},{"status":"passed","resource":"kube-system/kube-proxy-pjqlj"},{"status":"passed","resource":"kube-system/kube-proxy-vntdm"},{"status":"passed","resource":"kube-system/kube-scheduler-kv-master-0"}],"result":{"status":"failed","passed":33,"total":40}},{"version":1,"platform":"K8S","category":"Management and Governance","resource":"Pods","title":"Ensure all pods reference container images from known sources","description":"By default, Kubernetes allows users with the ability to create pods to reference any container image path, including public registries like DockerHub.  This allows developers to share and use pre-made container images easily, but it enables unvalidated and untrusted code to run inside your cluster with potential access to mounted secrets and service account tokens.  Container images should be verified to be conformant to security standards before being run, and the first step to this is to validate that all container images are being pulled from a known set of registries.  This helps development teams and security teams work from the same base location for running and validating images.","remediation":"Review all deployments and pod specifications, and find any that reference non-approved container registries.  Create a dedicated container registry in your environment, validate those container images meet your security policies, and store/mirror them to that dedicated container registry/registries.  Consider enforcing image sources early with a validation step in the CI/CD pipeline and enforcing the policy with OPA/Gatekeeper or other policy-based admission controller inside the cluster.","validation":"Run `kubectl get po -A -ojsonpath='{..image}' | kubectl get pods --all-namespaces -o jsonpath='{..image}' |tr -s '[[:space:]]' '\n' | sort | uniq -c ` and ensure all images are sourced from the official Kubernetes or cloud provider registries and your own internal container registries.","severity":0.8,"effort":0.5,"references":[{"url":"https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/","ref":"Kubectl List Images"}],"resources":[{"status":"failed","resource":"default/kube-bench-6vplv[aquasec/kube-bench:latest]"},{"status":"failed","resource":"default/kube-bench-d5jx2[aquasec/kube-bench:latest]"},{"status":"failed","resource":"default/kube-bench-g9gwh[aquasec/kube-bench:latest]"},{"status":"failed","resource":"default/kube-bench-mqzkw[aquasec/kube-bench:latest]"},{"status":"failed","resource":"default/kube-hunter-json-4bsvf[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-4hwjm[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-7z9wx[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-d5zj7[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-f74gb[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-f8xn5[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-g8bst[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-gqr7n[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-gzbsd[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-jfcws[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-jqptd[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-jv4j5[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-kjcjr[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-n2l5v[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-pftp6[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-psfgg[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-q6hp9[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-s2lpv[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-test-q8gt8[aquasec/kube-hunter]"},{"status":"failed","resource":"default/kube-hunter-json-zfprc[aquasec/kube-hunter]"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-5sfmz[nginx:latest]"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-h8lzn[nginx:latest]"},{"status":"failed","resource":"development/nginx-deployment-cc7df4f8f-p4r2d[nginx:latest]"},{"status":"failed","resource":"kube-system/calico-kube-controllers-dc4469c7f-d4rmp[calico/kube-controllers:v3.10.3]"},{"status":"failed","resource":"kube-system/calico-node-65r2f[calico/node:v3.10.3]"},{"status":"failed","resource":"kube-system/calico-node-kqqc6[calico/node:v3.10.3]"},{"status":"failed","resource":"kube-system/calico-node-lhqpq[calico/node:v3.10.3]"}],"result":{"status":"failed","passed":0,"total":31}},{"version":1,"platform":"K8S","category":"Network Access Control","resource":"Daemonsets","title":"Validate NetworkPolicy-aware enforcement is installed","description":"In GKE and EKS, the supported agent that can implement enforcement of `NetworkPolicy` resources is `Calico`, by Tigera.  In AKS, either `Calico` or AKS' own `azure` addon can implement micro-segmentation.  All of them are not enabled/installed by default and require explicit configuration.  In addition, the Kubernetes API will store `NetworkPolicy` resources, but without enforcement agents running, those never get applied on the nodes and pods which might cause a false sense of security.","remediation":"In GKE, enable Network Policy addon support.  In EKS, install the correct version of Calico for your version of EKS.  For AKS, configure either the `azure` or `calico` network policy addon.","validation":"Run `kubectl get daemonsets -n kube-system` and look for either `calico-node` or `azure-cni-networkmonitor` to be present.","severity":0.8,"effort":0.5,"references":[{"url":"https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#enabling_network_policy_enforcement","ref":"GKE Network Policy"},{"url":"https://docs.aws.amazon.com/eks/latest/userguide/calico.html","ref":"EKS Network Policy"},{"url":"https://docs.microsoft.com/en-us/azure/aks/use-network-policies","ref":"AKS Network Policy"}],"resources":[{"status":"passed","resource":"NetworkPolicy Daemonset"}],"result":{"status":"passed","passed":1,"total":1}},{"version":1,"platform":"K8S","category":"Network Access Control","resource":"NetworkPolicies","title":"Validate NetworkPolicies are defined in each namespace","description":"While support for `NetworkPolicies` is required in each cluster, the default policy allows all ingress and egress traffic to each pod.  Each namespace should have one or more `NetworkPolicy` resources defined to explicitly grant all ingress and egress access and to deny all other traffic.  Proper network access control at the pod level significantly reduces the ability for an attacker who has compromised a pod to move laterally to attack other pods or externally to instance metadata or cloud APIs.","remediation":"Deploy one or more `NetworkPolicy` resources in each namespace.  The most secure approach is a `default-deny-all` policy that blocks all ingress and egress traffic for that namespace followed by individual policies that allow the explicit traffic necessary.","validation":"Run `kubectl get networkpolicies --all-namespaces` and ensure each namespace has the desired policies defined.","severity":0.8,"effort":0.5,"references":[{"url":"https://kubernetes.io/docs/concepts/services-networking/network-policies/","ref":"Kubernetes Network Policies"},{"url":"https://github.com/ahmetb/kubernetes-network-policy-recipes","ref":"Kubernetes Example Network Policies"}],"resources":[{"status":"failed","resource":"default"},{"status":"failed","resource":"development"},{"status":"failed","resource":"kube-system"},{"status":"failed","resource":"production"}],"result":{"status":"failed","passed":0,"total":4}},{"version":1,"platform":"K8S","category":"Workload Isolation","resource":"Pods","title":"Ensure resource specification enforcement is installed","description":"Kubernetes RBAC determines who can create/read/update/delete resources.  However, users with the RBAC permission to create pods, for example, can define a pod specification that allows for direct access to the underlying worker nodes.  This can and has led to privilege escalation by attacking other workloads after escaping their container.  Natively, the `PodSecurityPolicy` admission controller allows administrators to define policies for pod specifications to prevent them from running as root, accessing the node's filesystem, running as a `privileged` container, and more.  Effectively configured `PodSecurityPolicies` can greatly reduce the negative effects of a malicious workload, but it is limited to just resources that define `pod` specifications and `templates`.  Solutions like OPA/Gatekeeper and K-rail leverage the `ValidatingWebhookConfiguration` resource in the API server to allow external applications the ability to apply custom logic to a given request to create/update a resource and allow or deny that request.  These deployments can validate the configuration of any resource (including pod specifications) with the appropriate policies in place.","remediation":"For basic needs limited to pod specification enforcement, consider enabling the `PodSecurityPolicy` admission controller and defining policies that do not allow privileged settings.  For intermediate to advanced use cases, consider deploying OPA/Gatekeeper or K-rail inside the cluster and define policies that enforce similar constraints on `pods` as well as other resources as needed.","validation":"Run `kubectl get psps --all-namespaces` to identify `PodSecurityPolicy` resources in place, or run `kubectl get deployments --all-namespaces` and look for `gatekeeper` or `k-rail` deployments to be present.","severity":0.9,"effort":0.7,"references":[{"url":"https://github.com/open-policy-agent/gatekeeper","ref":"OPA/Gatekeeper"},{"url":"https://github.com/cruise-automation/k-rail","ref":"K-rail"},{"url":"https://kubernetes.io/docs/concepts/policy/pod-security-policy/","ref":"Kubernetes PodSecurityPolicy"},{"url":"https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies","ref":"GKE PodSecurityPolicy"},{"url":"https://docs.aws.amazon.com/eks/latest/userguide/pod-security-policy.html","ref":"GKE PodSecurityPolicy"},{"url":"https://docs.microsoft.com/en-us/azure/aks/use-pod-security-policies","ref":"AKS PodSecurityPolicy"}],"resources":[{"status":"failed","resource":"OPA/Gatekeeper"},{"status":"failed","resource":"K-rail"},{"status":"failed","resource":"PodSecurityPolicy"}],"result":{"status":"failed","passed":0,"total":3}}]}
